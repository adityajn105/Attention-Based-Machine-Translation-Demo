{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention-Based-Machine-Translation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2fxjo8R6ncI",
        "colab_type": "text"
      },
      "source": [
        "### This tutorial is inspired from Coursera Deep Learning Specialization\n",
        "#### We will use faker for generating Fake Dates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulXH0oYxffrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"https://jscriptcoder.github.io/date-translator/Machine%20Translation%20with%20Attention%20model.html\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ptmk4sx34kGM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1974e0dd-6284-43a9-8d9f-98c7db131947"
      },
      "source": [
        "!pip -q install faker\n",
        "from faker import Faker\n",
        "import numpy as np\n",
        "import random\n",
        "from babel.dates import format_date"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▍                               | 10kB 24.0MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |███▊                            | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▏                           | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 337kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 348kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 358kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 368kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 378kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 389kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 399kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 409kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 419kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 430kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 440kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 450kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 460kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 471kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 481kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 491kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 501kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 512kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 522kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 532kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 542kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 552kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 563kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 573kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 583kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 593kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 604kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 614kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 624kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 634kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 645kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 655kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 665kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 675kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 686kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 696kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 706kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 716kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 727kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 737kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 747kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 757kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 768kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 778kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 788kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 798kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 808kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 819kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 829kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 839kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 849kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 860kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 870kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 880kB 3.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQSr1YWk4rlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faker = Faker()\n",
        "faker.seed(5)\n",
        "np.random.seed(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hQQa7YG7IFA",
        "colab_type": "code",
        "outputId": "524045da-8760-4ccd-9e5e-971a05074fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "#these are the date formats we are going to generate\n",
        "FORMATS = ['short','medium','medium','medium','long','long','long','long','long','full','full','full','d MMM YYY','d MMMM YYY','d MMMM YYY',\n",
        "           'd MMMM YYY','d MMMM YYY','d MMMM YYY','dd/MM/YYY','EE d, MMM YYY','EEEE d, MMMM YYY','MMM d, YYY','MMMM d, YYY','YYY, d MMM','YYY, d MMMM']\n",
        "for format in FORMATS:\n",
        "    print('%s => %s' %(format, format_date(faker.date_object(), format=format, locale='en')))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "short => 12/10/72\n",
            "medium => Mar 27, 1973\n",
            "medium => Jan 29, 1983\n",
            "medium => Jun 21, 2017\n",
            "long => July 26, 1972\n",
            "long => February 27, 2005\n",
            "long => July 8, 1989\n",
            "long => September 28, 1976\n",
            "long => May 31, 2019\n",
            "full => Tuesday, October 8, 2002\n",
            "full => Thursday, May 12, 2005\n",
            "full => Thursday, December 29, 2005\n",
            "d MMM YYY => 22 Jan 1997\n",
            "d MMMM YYY => 5 September 1998\n",
            "d MMMM YYY => 5 February 1987\n",
            "d MMMM YYY => 15 July 1996\n",
            "d MMMM YYY => 9 October 1970\n",
            "d MMMM YYY => 22 May 2013\n",
            "dd/MM/YYY => 07/01/1995\n",
            "EE d, MMM YYY => Tue 25, Oct 1983\n",
            "EEEE d, MMMM YYY => Saturday 6, August 2016\n",
            "MMM d, YYY => Jan 10, 2006\n",
            "MMMM d, YYY => July 18, 1997\n",
            "YYY, d MMM => 1974, 15 Oct\n",
            "YYY, d MMMM => 2002, 25 July\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCW4AFQy-Tg_",
        "colab_type": "code",
        "outputId": "f8a84b49-4c3f-4c53-e2c8-071359e3f6f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def random_date():\n",
        "    dt = faker.date_object()\n",
        "    try:\n",
        "        date = format_date(dt, format=random.choice(FORMATS), locale='en')\n",
        "        human_readable = date.lower().replace(',', '')\n",
        "        machine_readable = dt.isoformat()\n",
        "    except AttributeError as e:\n",
        "        return None, None, None\n",
        "    return human_readable, machine_readable\n",
        "random_date()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('dec 27 1972', '1972-12-27')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKEZyFaQ_HZd",
        "colab_type": "code",
        "outputId": "131c8eaf-0ffa-46aa-d60b-cf1193b2d168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "human_vocab = set()\n",
        "machine_vocab = set()\n",
        "dataset = []\n",
        "m = 50000\n",
        "for i in range(m):\n",
        "  hd,md = random_date()\n",
        "  dataset.append((hd,md))\n",
        "  human_vocab.update( tuple(hd) )\n",
        "  machine_vocab.update( tuple(md) )\n",
        "  \n",
        "human_vocab.add('<pad>')\n",
        "human_vocab = dict(enumerate(human_vocab))\n",
        "human_vocab = { v:i for i,v in human_vocab.items()  }\n",
        "\n",
        "machine_vocab = dict(enumerate(machine_vocab))\n",
        "inv_machine_vocab = { v:i for i,v in machine_vocab.items()}\n",
        "\n",
        "print(len(dataset),len(human_vocab),len(machine_vocab))\n",
        "dataset[:10]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 35 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('may 4 1997', '1997-05-04'),\n",
              " ('9 april 1980', '1980-04-09'),\n",
              " ('march 6 2018', '2018-03-06'),\n",
              " ('oct 19 1989', '1989-10-19'),\n",
              " ('wednesday september 12 1979', '1979-09-12'),\n",
              " ('wednesday may 22 2013', '2013-05-22'),\n",
              " ('1973 24 january', '1973-01-24'),\n",
              " ('5 april 2013', '2013-04-05'),\n",
              " ('11/07/2002', '2002-07-11'),\n",
              " ('dec 3 1970', '1970-12-03')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_PZwK7tWJVJ",
        "colab_type": "code",
        "outputId": "23d0d586-de2e-489a-ec83-cf11d037dbe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "HUMAN_VOCAB = len(human_vocab)\n",
        "MACHINE_VOCAB = len(machine_vocab)\n",
        "Tx = 30\n",
        "Ty = 10\n",
        "print( HUMAN_VOCAB, MACHINE_VOCAB )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esagXwG-VmEF",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Converting Human readable dates to character vectors\n",
        "#### 2. Converting Machine Dates to character vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgT1M1o4ah57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_to_ohe( string, T, vocab ):\n",
        "  string = string.lower()\n",
        "  arr = []\n",
        "  while len(arr) < len(string):\n",
        "    arr.append( vocab[ string[len(arr)] ] )\n",
        "  while len(arr) < T:\n",
        "    arr.append( vocab['<pad>'] )\n",
        "    \n",
        "  onehot = np.zeros( (T,len(vocab)) )\n",
        "  for i in range(T):\n",
        "    onehot[ i, arr[i] ] = 1\n",
        "  return onehot, arr\n",
        "\n",
        "def output_to_date( out, vocab ):\n",
        "  arr = np.argmax(out,axis=-1)\n",
        "  string = ''\n",
        "  for i in arr:\n",
        "    string += vocab[ i ]\n",
        "  return string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIzu5evxZiHq",
        "colab_type": "code",
        "outputId": "a9c40931-3b4a-443c-8513-865b193774f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "for x,y in dataset:\n",
        "  X.append( string_to_ohe(x, Tx, human_vocab)[0] )\n",
        "  Y.append( string_to_ohe(y, Ty, inv_machine_vocab)[0] )\n",
        "X,Y = np.array(X), np.array(Y)\n",
        "X.shape, Y.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 30, 35), (50000, 10, 11))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr4noLFaoI41",
        "colab_type": "text"
      },
      "source": [
        "## Defining Attention Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhJuSpFAqbsi",
        "colab_type": "text"
      },
      "source": [
        "| Overall | Attention Mechanism |\n",
        "|-------------|------------------------|\n",
        "| ![alt text](https://github.com/adityajn105/Coursera-Deep-Learning-Specialization/raw/26cf7da29b2f1cb32799e045cc9cdfab99ad0757/4.%20Sequence%20Models/Week%203/Machine%20Translation/images/attn_model.png) | ![alt text](https://raw.githubusercontent.com/adityajn105/Coursera-Deep-Learning-Specialization/26cf7da29b2f1cb32799e045cc9cdfab99ad0757/4.%20Sequence%20Models/Week%203/Machine%20Translation/images/attn_mechanism.png) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc4eJYBDe00L",
        "colab_type": "text"
      },
      "source": [
        "* The post-attention LSTM passes $s^{\\langle t \\rangle}, c^{\\langle t \\rangle}$ from one time step to the next.\n",
        "* in this model the post-attention LSTM at time $t$ does will not take the specific generated $y^{\\langle t-1 \\rangle}$ as input; it only takes $s^{\\langle t\\rangle}$ and $c^{\\langle t\\rangle}$ as input. \n",
        "* We use $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}; \\overleftarrow{a}^{\\langle t \\rangle}]$ to represent the concatenation of the activations of both the forward-direction and backward-directions of the pre-attention Bi-LSTM.\n",
        "* The diagram on the right uses a RepeatVector node to copy $s^{\\langle t-1 \\rangle}$'s value $T_x$ times, and then Concatenation to concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ to compute $e^{\\langle t, t'}$, which is then passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$. We'll explain how to use RepeatVector and Concatenation in Keras below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2In8L2LBmyRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import RepeatVector, Concatenate, Dense, Dot, Activation\n",
        "#combines weights generated from BiLSTM with previous state of Post LSTM cell to get attention to be given to each timestep\n",
        "#heart of attention model\n",
        "def one_step_attention( a, s_prev ):\n",
        "  x = RepeatVector(Tx)(s_prev)             #repeat s_prev Tx times\n",
        "  x = Concatenate(axis=-1)( [ a, x ] )     #concat each copy of s_prev with each timestep hidden state\n",
        "  e = Dense(10, activation='tanh')(x)      #pass each concatenated vector through Dense Layer to get intermediate energies\n",
        "  energy = Dense(1, activation='relu')(e)  #get energy \n",
        "  alphas = Activation('softmax')(energy)   #convert energy to probabilities i.e. attention weights\n",
        "  context = Dot(axes=1)([alphas,a])        #multiply attention weights and timestep hidden state to get context vector\n",
        "  return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANS4c96Goy-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Bidirectional, LSTM\n",
        "from keras.models import Model\n",
        "\n",
        "n_a = 32 #pre attention LSTM state, since Bi directional attention=64\n",
        "n_s = 64 #post attention LSTM state\n",
        "\n",
        "inp = Input( (Tx, HUMAN_VOCAB ) )\n",
        "s0 = Input( (n_s,) )\n",
        "c0 = Input( (n_s,) )\n",
        "\n",
        "outputs = []\n",
        "\n",
        "s=s0\n",
        "c=c0\n",
        "a = Bidirectional( LSTM( n_a, return_sequences=True ) )(inp) #generate hidden state for every timestep\n",
        "\n",
        "\"https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\"\n",
        "postLSTM = LSTM( n_s, return_state = True)\n",
        "\n",
        "output = Dense( MACHINE_VOCAB, activation='softmax') #our final output layer\n",
        "\n",
        "for _ in range(Ty): #iterate for every output step\n",
        "  context = one_step_attention(a, s) #get context\n",
        "  s,_,c = postLSTM(context, initial_state=[s,c]) #generate\n",
        "  out = output(s) \n",
        "  outputs.append(out)\n",
        "  \n",
        "model = Model( [inp,s0,c0], outputs )\n",
        "#model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MwhPm4ovKqw",
        "colab_type": "code",
        "outputId": "8b5aabd3-fe32-43a4-d40d-cf9e746a4c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1057
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "model.compile( optimizer=Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01), loss='categorical_crossentropy', metrics=['accuracy'] )\n",
        "\n",
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "\n",
        "Y = list(Y.swapaxes(0,1))\n",
        "\n",
        "history = model.fit( [X,s0,c0], Y, epochs=30, batch_size=128, verbose=1)\n",
        "model.save_weights('attention_weights.h5')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "50000/50000 [==============================] - 65s 1ms/step - loss: 8.6328 - dense_22_loss: 1.7779 - dense_22_acc: 0.9414 - dense_22_acc_1: 0.9388 - dense_22_acc_2: 0.7549 - dense_22_acc_3: 0.4464 - dense_22_acc_4: 0.9367 - dense_22_acc_5: 0.7460 - dense_22_acc_6: 0.4862 - dense_22_acc_7: 0.9159 - dense_22_acc_8: 0.5206 - dense_22_acc_9: 0.3298\n",
            "Epoch 2/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 2.8699 - dense_22_loss: 0.8936 - dense_22_acc: 0.9894 - dense_22_acc_1: 0.9906 - dense_22_acc_2: 0.8868 - dense_22_acc_3: 0.8300 - dense_22_acc_4: 0.9998 - dense_22_acc_5: 0.9834 - dense_22_acc_6: 0.9095 - dense_22_acc_7: 0.9998 - dense_22_acc_8: 0.8140 - dense_22_acc_9: 0.7002\n",
            "Epoch 3/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 1.8533 - dense_22_loss: 0.5443 - dense_22_acc: 0.9917 - dense_22_acc_1: 0.9923 - dense_22_acc_2: 0.9085 - dense_22_acc_3: 0.8791 - dense_22_acc_4: 0.9999 - dense_22_acc_5: 0.9862 - dense_22_acc_6: 0.9470 - dense_22_acc_7: 0.9999 - dense_22_acc_8: 0.8890 - dense_22_acc_9: 0.8348\n",
            "Epoch 4/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4544 - dense_22_loss: 0.4181 - dense_22_acc: 0.9930 - dense_22_acc_1: 0.9935 - dense_22_acc_2: 0.9249 - dense_22_acc_3: 0.8996 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9869 - dense_22_acc_6: 0.9572 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9185 - dense_22_acc_9: 0.8661\n",
            "Epoch 5/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 1.2298 - dense_22_loss: 0.3543 - dense_22_acc: 0.9935 - dense_22_acc_1: 0.9942 - dense_22_acc_2: 0.9374 - dense_22_acc_3: 0.9171 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9878 - dense_22_acc_6: 0.9612 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9336 - dense_22_acc_9: 0.8816\n",
            "Epoch 6/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 1.0772 - dense_22_loss: 0.3131 - dense_22_acc: 0.9942 - dense_22_acc_1: 0.9946 - dense_22_acc_2: 0.9502 - dense_22_acc_3: 0.9333 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9884 - dense_22_acc_6: 0.9639 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9443 - dense_22_acc_9: 0.8924\n",
            "Epoch 7/30\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.9626 - dense_22_loss: 0.2816 - dense_22_acc: 0.9947 - dense_22_acc_1: 0.9949 - dense_22_acc_2: 0.9581 - dense_22_acc_3: 0.9446 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9887 - dense_22_acc_6: 0.9654 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9526 - dense_22_acc_9: 0.9026\n",
            "Epoch 8/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.8685 - dense_22_loss: 0.2547 - dense_22_acc: 0.9950 - dense_22_acc_1: 0.9952 - dense_22_acc_2: 0.9633 - dense_22_acc_3: 0.9534 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9893 - dense_22_acc_6: 0.9664 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9609 - dense_22_acc_9: 0.9139\n",
            "Epoch 9/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.7902 - dense_22_loss: 0.2306 - dense_22_acc: 0.9956 - dense_22_acc_1: 0.9954 - dense_22_acc_2: 0.9689 - dense_22_acc_3: 0.9596 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9895 - dense_22_acc_6: 0.9676 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9662 - dense_22_acc_9: 0.9240\n",
            "Epoch 10/30\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.7228 - dense_22_loss: 0.2084 - dense_22_acc: 0.9956 - dense_22_acc_1: 0.9956 - dense_22_acc_2: 0.9724 - dense_22_acc_3: 0.9642 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9900 - dense_22_acc_6: 0.9685 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9716 - dense_22_acc_9: 0.9331\n",
            "Epoch 11/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.6660 - dense_22_loss: 0.1896 - dense_22_acc: 0.9958 - dense_22_acc_1: 0.9958 - dense_22_acc_2: 0.9760 - dense_22_acc_3: 0.9679 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9900 - dense_22_acc_6: 0.9692 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9747 - dense_22_acc_9: 0.9403\n",
            "Epoch 12/30\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.6173 - dense_22_loss: 0.1746 - dense_22_acc: 0.9960 - dense_22_acc_1: 0.9959 - dense_22_acc_2: 0.9786 - dense_22_acc_3: 0.9720 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9902 - dense_22_acc_6: 0.9698 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9765 - dense_22_acc_9: 0.9443\n",
            "Epoch 13/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.5753 - dense_22_loss: 0.1621 - dense_22_acc: 0.9962 - dense_22_acc_1: 0.9962 - dense_22_acc_2: 0.9812 - dense_22_acc_3: 0.9748 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9902 - dense_22_acc_6: 0.9705 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9789 - dense_22_acc_9: 0.9489\n",
            "Epoch 14/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.5423 - dense_22_loss: 0.1520 - dense_22_acc: 0.9963 - dense_22_acc_1: 0.9963 - dense_22_acc_2: 0.9835 - dense_22_acc_3: 0.9775 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9907 - dense_22_acc_6: 0.9706 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9794 - dense_22_acc_9: 0.9514\n",
            "Epoch 15/30\n",
            "50000/50000 [==============================] - 54s 1ms/step - loss: 0.5111 - dense_22_loss: 0.1432 - dense_22_acc: 0.9964 - dense_22_acc_1: 0.9964 - dense_22_acc_2: 0.9854 - dense_22_acc_3: 0.9794 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9909 - dense_22_acc_6: 0.9717 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9807 - dense_22_acc_9: 0.9539\n",
            "Epoch 16/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.4848 - dense_22_loss: 0.1354 - dense_22_acc: 0.9964 - dense_22_acc_1: 0.9965 - dense_22_acc_2: 0.9870 - dense_22_acc_3: 0.9812 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9908 - dense_22_acc_6: 0.9719 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9814 - dense_22_acc_9: 0.9564\n",
            "Epoch 17/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.4601 - dense_22_loss: 0.1282 - dense_22_acc: 0.9966 - dense_22_acc_1: 0.9966 - dense_22_acc_2: 0.9889 - dense_22_acc_3: 0.9825 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9909 - dense_22_acc_6: 0.9724 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9819 - dense_22_acc_9: 0.9588\n",
            "Epoch 18/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.4405 - dense_22_loss: 0.1227 - dense_22_acc: 0.9968 - dense_22_acc_1: 0.9968 - dense_22_acc_2: 0.9899 - dense_22_acc_3: 0.9841 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9913 - dense_22_acc_6: 0.9728 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9828 - dense_22_acc_9: 0.9600\n",
            "Epoch 19/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.4221 - dense_22_loss: 0.1181 - dense_22_acc: 0.9968 - dense_22_acc_1: 0.9968 - dense_22_acc_2: 0.9913 - dense_22_acc_3: 0.9848 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9912 - dense_22_acc_6: 0.9731 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9836 - dense_22_acc_9: 0.9616\n",
            "Epoch 20/30\n",
            "50000/50000 [==============================] - 54s 1ms/step - loss: 0.4057 - dense_22_loss: 0.1130 - dense_22_acc: 0.9968 - dense_22_acc_1: 0.9969 - dense_22_acc_2: 0.9919 - dense_22_acc_3: 0.9858 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9913 - dense_22_acc_6: 0.9736 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9840 - dense_22_acc_9: 0.9630\n",
            "Epoch 21/30\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.3898 - dense_22_loss: 0.1089 - dense_22_acc: 0.9969 - dense_22_acc_1: 0.9971 - dense_22_acc_2: 0.9930 - dense_22_acc_3: 0.9867 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9912 - dense_22_acc_6: 0.9740 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9845 - dense_22_acc_9: 0.9645\n",
            "Epoch 22/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.3772 - dense_22_loss: 0.1055 - dense_22_acc: 0.9971 - dense_22_acc_1: 0.9972 - dense_22_acc_2: 0.9932 - dense_22_acc_3: 0.9876 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9916 - dense_22_acc_6: 0.9747 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9849 - dense_22_acc_9: 0.9649\n",
            "Epoch 23/30\n",
            "50000/50000 [==============================] - 57s 1ms/step - loss: 0.3653 - dense_22_loss: 0.1022 - dense_22_acc: 0.9969 - dense_22_acc_1: 0.9973 - dense_22_acc_2: 0.9936 - dense_22_acc_3: 0.9883 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9916 - dense_22_acc_6: 0.9745 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9850 - dense_22_acc_9: 0.9667\n",
            "Epoch 24/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.3541 - dense_22_loss: 0.0991 - dense_22_acc: 0.9971 - dense_22_acc_1: 0.9973 - dense_22_acc_2: 0.9941 - dense_22_acc_3: 0.9887 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9917 - dense_22_acc_6: 0.9754 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9859 - dense_22_acc_9: 0.9676\n",
            "Epoch 25/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.3433 - dense_22_loss: 0.0965 - dense_22_acc: 0.9974 - dense_22_acc_1: 0.9975 - dense_22_acc_2: 0.9945 - dense_22_acc_3: 0.9896 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9920 - dense_22_acc_6: 0.9758 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9861 - dense_22_acc_9: 0.9676\n",
            "Epoch 26/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.3349 - dense_22_loss: 0.0937 - dense_22_acc: 0.9972 - dense_22_acc_1: 0.9977 - dense_22_acc_2: 0.9949 - dense_22_acc_3: 0.9899 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9917 - dense_22_acc_6: 0.9756 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9863 - dense_22_acc_9: 0.9682\n",
            "Epoch 27/30\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.3259 - dense_22_loss: 0.0916 - dense_22_acc: 0.9972 - dense_22_acc_1: 0.9976 - dense_22_acc_2: 0.9950 - dense_22_acc_3: 0.9901 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9919 - dense_22_acc_6: 0.9762 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9867 - dense_22_acc_9: 0.9688\n",
            "Epoch 28/30\n",
            "50000/50000 [==============================] - 56s 1ms/step - loss: 0.3178 - dense_22_loss: 0.0893 - dense_22_acc: 0.9974 - dense_22_acc_1: 0.9978 - dense_22_acc_2: 0.9952 - dense_22_acc_3: 0.9907 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9919 - dense_22_acc_6: 0.9768 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9871 - dense_22_acc_9: 0.9706\n",
            "Epoch 29/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.3106 - dense_22_loss: 0.0873 - dense_22_acc: 0.9973 - dense_22_acc_1: 0.9979 - dense_22_acc_2: 0.9955 - dense_22_acc_3: 0.9909 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9921 - dense_22_acc_6: 0.9769 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9874 - dense_22_acc_9: 0.9707\n",
            "Epoch 30/30\n",
            "50000/50000 [==============================] - 55s 1ms/step - loss: 0.3035 - dense_22_loss: 0.0855 - dense_22_acc: 0.9976 - dense_22_acc_1: 0.9980 - dense_22_acc_2: 0.9955 - dense_22_acc_3: 0.9915 - dense_22_acc_4: 1.0000 - dense_22_acc_5: 0.9921 - dense_22_acc_6: 0.9770 - dense_22_acc_7: 1.0000 - dense_22_acc_8: 0.9876 - dense_22_acc_9: 0.9716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kPPsNOpkeKp",
        "colab_type": "code",
        "outputId": "f5f8451c-0377-49d7-8290-6919515c8539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "model.load_weights('attention_weights.h5')\n",
        "def getTranslation(date,model):\n",
        "  date = date.lower().replace(',','')\n",
        "  source = np.array(string_to_ohe(date, Tx, human_vocab)[0])\n",
        "  source = np.expand_dims(source,axis=0)\n",
        "  prediction = np.array(model.predict([source, s0, c0]))\n",
        "  prediction = np.squeeze(prediction.swapaxes(0,1))\n",
        "  return output_to_date(prediction,machine_vocab)\n",
        "\n",
        "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', \n",
        "            '1 March 2001','jun 10 2017','11/07/2002']\n",
        "\n",
        "for example in EXAMPLES:\n",
        "    print(f\"{example} -> {getTranslation(example,model)}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 May 1979 -> 1979-05-03\n",
            "5 April 09 -> 2009-04-04\n",
            "21th of August 2016 -> 2016-08-22\n",
            "Tue 10 Jul 2007 -> 2007-07-10\n",
            "Saturday May 9 2018 -> 2018-05-09\n",
            "March 3 2001 -> 2001-03-03\n",
            "March 3rd 2001 -> 2001-03-03\n",
            "1 March 2001 -> 2001-03-01\n",
            "jun 10 2017 -> 2017-06-10\n",
            "11/07/2002 -> 2002-07-17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSN9CyE8yiR9",
        "colab_type": "code",
        "outputId": "8d9cdc9f-23df-4962-b345-fe58e0ac858d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "done = False\n",
        "while not done:\n",
        "  dt = input(\"Enter Date : \")\n",
        "  print(f\"Translation : {getTranslation(dt,model)}     Continue('y/n') :\",end=\"\")\n",
        "  done = input() == 'n'"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter Date : 21 july 1976\n",
            "Translation : 1976-07-21     Continue('y/n') :y\n",
            "Enter Date : 11/07/2002\n",
            "Translation : 2002-07-17     Continue('y/n') :n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhl_092Y6DAY",
        "colab_type": "code",
        "outputId": "c53200fa-c0a3-4e69-aa20-9e222c1b9af1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3366
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 30, 35)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 30, 64)       17408       input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_11 (RepeatVector) (None, 30, 64)       0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 30, 128)      0           bidirectional_2[0][0]            \n",
            "                                                                 repeat_vector_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 30, 10)       1290        concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 30, 1)        11          dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 30, 1)        0           dense_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_11 (Dot)                    (None, 1, 64)        0           activation_11[0][0]              \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, 64), (None,  33024       dot_11[0][0]                     \n",
            "                                                                 input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "                                                                 dot_12[0][0]                     \n",
            "                                                                 lstm_4[0][0]                     \n",
            "                                                                 lstm_4[0][2]                     \n",
            "                                                                 dot_13[0][0]                     \n",
            "                                                                 lstm_4[1][0]                     \n",
            "                                                                 lstm_4[1][2]                     \n",
            "                                                                 dot_14[0][0]                     \n",
            "                                                                 lstm_4[2][0]                     \n",
            "                                                                 lstm_4[2][2]                     \n",
            "                                                                 dot_15[0][0]                     \n",
            "                                                                 lstm_4[3][0]                     \n",
            "                                                                 lstm_4[3][2]                     \n",
            "                                                                 dot_16[0][0]                     \n",
            "                                                                 lstm_4[4][0]                     \n",
            "                                                                 lstm_4[4][2]                     \n",
            "                                                                 dot_17[0][0]                     \n",
            "                                                                 lstm_4[5][0]                     \n",
            "                                                                 lstm_4[5][2]                     \n",
            "                                                                 dot_18[0][0]                     \n",
            "                                                                 lstm_4[6][0]                     \n",
            "                                                                 lstm_4[6][2]                     \n",
            "                                                                 dot_19[0][0]                     \n",
            "                                                                 lstm_4[7][0]                     \n",
            "                                                                 lstm_4[7][2]                     \n",
            "                                                                 dot_20[0][0]                     \n",
            "                                                                 lstm_4[8][0]                     \n",
            "                                                                 lstm_4[8][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_12 (RepeatVector) (None, 30, 64)       0           lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 30, 128)      0           bidirectional_2[0][0]            \n",
            "                                                                 repeat_vector_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 30, 10)       1290        concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 30, 1)        11          dense_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 30, 1)        0           dense_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_12 (Dot)                    (None, 1, 64)        0           activation_12[0][0]              \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_13 (RepeatVector) (None, 30, 64)       0           lstm_4[1][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 30, 128)      0           bidirectional_2[0][0]            \n",
            "                                                                 repeat_vector_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 30, 10)       1290        concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 30, 1)        11          dense_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 30, 1)        0           dense_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_13 (Dot)                    (None, 1, 64)        0           activation_13[0][0]              \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_14 (RepeatVector) (None, 30, 64)       0           lstm_4[2][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 30, 128)      0           bidirectional_2[0][0]            \n",
            "                                                                 repeat_vector_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 30, 10)       1290        concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_30 (Dense)                (None, 30, 1)        11          dense_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 30, 1)        0           dense_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_14 (Dot)                    (None, 1, 64)        0           activation_14[0][0]              \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_15 (RepeatVector) (None, 30, 64)       0           lstm_4[3][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 30, 128)      0           bidirectional_2[0][0]            \n",
            "                                                                 repeat_vector_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 30, 10)       1290        concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_32 (Dense)                (None, 30, 1)        11          dense_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 30, 1)        0           dense_32[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_15 (Dot)                    (None, 1, 64)        0           activation_15[0][0]              \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_16 (RepeatVector) (None, 30, 64)       0           lstm_4[4][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 30, 128)      0           bidirectional_2[0][0]            \n",
            "                                                                 repeat_vector_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_33 (Dense)                (None, 30, 10)       1290        concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_34 (Dense)                (None, 30, 1)        11          dense_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 30, 1)        0           dense_34[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_16 (Dot)                    (None, 1, 64)        0           activation_16[0][0]              \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_17 (RepeatVector) (None, 30, 64)       0           lstm_4[5][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 30, 128)      0           bidirectional_2[0][0]            \n",
            "                                                                 repeat_vector_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_35 (Dense)                (None, 30, 10)       1290        concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_36 (Dense)                (None, 30, 1)        11          dense_35[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 30, 1)        0           dense_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_17 (Dot)                    (None, 1, 64)        0           activation_17[0][0]              \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_18 (RepeatVector) (None, 30, 64)       0           lstm_4[6][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 30, 128)      0           bidirectional_2[0][0]            \n",
            "                                                                 repeat_vector_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_37 (Dense)                (None, 30, 10)       1290        concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_38 (Dense)                (None, 30, 1)        11          dense_37[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 30, 1)        0           dense_38[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_18 (Dot)                    (None, 1, 64)        0           activation_18[0][0]              \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_19 (RepeatVector) (None, 30, 64)       0           lstm_4[7][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 30, 128)      0           bidirectional_2[0][0]            \n",
            "                                                                 repeat_vector_19[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_39 (Dense)                (None, 30, 10)       1290        concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_40 (Dense)                (None, 30, 1)        11          dense_39[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 30, 1)        0           dense_40[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_19 (Dot)                    (None, 1, 64)        0           activation_19[0][0]              \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_20 (RepeatVector) (None, 30, 64)       0           lstm_4[8][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 30, 128)      0           bidirectional_2[0][0]            \n",
            "                                                                 repeat_vector_20[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_41 (Dense)                (None, 30, 10)       1290        concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_42 (Dense)                (None, 30, 1)        11          dense_41[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 30, 1)        0           dense_42[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_20 (Dot)                    (None, 1, 64)        0           activation_20[0][0]              \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 11)           715         lstm_4[0][0]                     \n",
            "                                                                 lstm_4[1][0]                     \n",
            "                                                                 lstm_4[2][0]                     \n",
            "                                                                 lstm_4[3][0]                     \n",
            "                                                                 lstm_4[4][0]                     \n",
            "                                                                 lstm_4[5][0]                     \n",
            "                                                                 lstm_4[6][0]                     \n",
            "                                                                 lstm_4[7][0]                     \n",
            "                                                                 lstm_4[8][0]                     \n",
            "                                                                 lstm_4[9][0]                     \n",
            "==================================================================================================\n",
            "Total params: 64,157\n",
            "Trainable params: 64,157\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw18KUzFdg6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}